# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1McnZvgXeT1Y7vi0UC4tVH4KfsGWB72r7

Step 1 import nessary libraries
"""

# Load the libraries needed
import os
import cv2
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.metrics import davies_bouldin_score
import shutil

"""Unzipping the unlabelled data"""

# Unzip product_images.zip
shutil.unpack_archive("product_images.zip", "/content/")

"""Define a function to load images from a folder"""

# Load images from the folder
def load_images_from_folder(folder):
    images = []
    filenames = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder, filename))
        if img is not None:
            img = cv2.resize(img, (150, 150))
            images.append(img)
            filenames.append(filename)
    return np.array(images), filenames

"""Define a function to performa feature Ectraction using the pre-trained VGG16 Model"""

# Feature extraction using pre-trained VGG16 model
def extract_features(images):
    model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))
    model.trainable = False
    features = model.predict(preprocess_input(images))
    return features

"""Define a function to flatten the extracted Features"""

# Flatten the extracted features
def flatten_features(features):
    return features.reshape(features.shape[0], -1)

"""Define a function to evaluate the optimal number of clusters for K-Means clustering using the Davies-Bouldin Index"""

def davies_bouldin_method(features, max_clusters=20):
    davies_bouldin_scores = [] # Initialize an empty list to store Davies-Bouldin scores for different cluster numbers.
    for n_clusters in range(2, max_clusters + 1): # Loop through Cluster Numbers
        kmeans = KMeans(n_init=10, n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(features)
        db_score = davies_bouldin_score(features, labels)
        davies_bouldin_scores.append(db_score)
    plt.figure(figsize=(10, 5))
    plt.plot(range(2, max_clusters + 1), davies_bouldin_scores, marker='o')
    plt.xlabel('Number of clusters')
    plt.ylabel('Davies-Bouldin Index')
    plt.title('Davies-Bouldin Method')
    plt.xticks(range(2, max_clusters + 1))  # Set x-axis to show only integers
    plt.show()

"""Start Image clustering"""

# Load images
folder_path = '/content/product_images'
images, filenames = load_images_from_folder(folder_path)

"""Features are extracted from the loaded images using the VGG16 model."""

# Extract features
features = extract_features(images)

"""The extracted features are flattened into a 2D array."""

# Flatten features
flattened_features = flatten_features(features)

"""Apply PCA for Dimensionality Reduction"""

# Apply PCA for dimensionality reduction (optional but recommended for high-dimensional data)
pca = PCA(n_components=50)
reduced_features = pca.fit_transform(flattened_features)

"""Identify the optimal number of clusters"""

#  Identify the optimal number of clusters
davies_bouldin_method(reduced_features)

"""Perform K-Means Clustering"""

# Perform k-means clustering
n_clusters = 12
kmeans = KMeans(n_clusters, n_init=10, random_state=42)
kmeans.fit(reduced_features)
labels = kmeans.labels_

"""Step 7. Visualise the Clusters"""

# Visualise the clusters
for cluster in range(n_clusters):
    cluster_images = images[labels == cluster]
    plt.figure(figsize=(20, 20))
    for i in range(min(len(cluster_images), 25)):
        plt.subplot(5, 5, i + 1)
        plt.imshow(cluster_images[i])
        plt.axis('off')
    plt.suptitle(f'Cluster {cluster + 1}')
    plt.show()

"""Step 8. Export the results as a CSV file"""

# Export the results as a CSV file
df = pd.DataFrame({'Filename': filenames, 'Cluster': labels})
df.to_csv("clustering_results.csv", index=False)

from google.colab import drive
drive.mount('/content/drive')